import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader, PyPDFLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
import tempfile
import os


st.title("ğŸ“„ Assignment 15")
st.write(
    "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ GPTê°€ ë¬¸ì„œë¥¼ ì½ê³  ë‹µì„ ë“œë ¤ìš”"
    "OpenAPI ì‚¬ìš©ì„ ìœ„í•´ Settingsì— API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”"
)
cache_dir = LocalFileStore("./.cache/")

with st.sidebar:
    st.title("Settings")
    st.markdown("***")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.", icon="ğŸ—ï¸")

    uploaded_file = st.file_uploader(
        "ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš” (.txt, .pdf, .docx)", type=("txt", "pdf", "docx")
    )
    st.markdown("***")
    st.link_button(
        "Github Repo ë°”ë¡œê°€ê¸°", "https://github.com/asuracoder91/streamlit_rag"
    )

question = st.text_area(
    "ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ í•˜ì„¸ìš”",
    placeholder="ë¬¸ì„œ ìš”ì•½ì¢€ í•´ì¤„ë˜?",
    disabled=not uploaded_file,
)

if openai_api_key and uploaded_file and question:
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_file_path = tmp_file.name

    llm = ChatOpenAI(api_key=openai_api_key, model="gpt-4o-mini", temperature=0.1)

    # íŒŒì¼ í˜•ì‹ì— ë§ëŠ” ë¡œë” ì„ íƒ
    file_extension = os.path.splitext(uploaded_file.name)[1].lower()

    if file_extension == ".pdf":
        loader = PyPDFLoader(tmp_file_path)  # PDF íŒŒì¼ ì²˜ë¦¬
    elif file_extension == ".txt":
        loader = TextLoader(tmp_file_path)  # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
    elif file_extension == ".docx":
        loader = UnstructuredFileLoader(tmp_file_path)  # DOCX íŒŒì¼ ì²˜ë¦¬
    else:
        st.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        st.stop()

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )

    docs = loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()

    prompt_template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\n\n{context}, and say in Korean Only.",
            ),
            ("human", "{question}"),
        ]
    )

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    def chain_with_memory(question):
        chat_history = memory.load_memory_variables({}).get("chat_history", "")

        relevant_docs = retriever.get_relevant_documents(question)
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        inputs = {
            "context": context,
            "question": question,
            "chat_history": chat_history,
        }

        prompt = prompt_template.format_messages(**inputs)
        response = llm(prompt)

        memory.save_context({"question": question}, {"answer": response.content})

        return response.content

    if st.button("ì§ˆë¬¸ì— ë‹µë³€ ë°›ê¸°"):
        with st.spinner("GPTê°€ ë¬¸ì„œë¥¼ ì½ê³  ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
            answer = chain_with_memory(question)
            st.write("### ë‹µë³€:")
            st.write(answer)

# ì˜¤ë¥˜ ë˜ëŠ” ì•Œë¦¼ ì²˜ë¦¬
elif not openai_api_key:
    st.warning("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
elif not uploaded_file:
    st.info("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
elif not question:
    st.info("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
